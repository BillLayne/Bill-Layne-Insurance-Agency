{
  "permissions": {
    "allow": [
      "Bash(git add:*)",
      "Bash(git commit:*)",
      "Bash(git push:*)",
      "Bash(grep:*)",
      "mcp__Desktop_Commander__list_directory",
      "mcp__Desktop_Commander__read_file",
      "Bash(awk:*)",
      "Bash(perl:*)",
      "Bash(dd:*)",
      "mcp__Desktop_Commander__edit_block",
      "Bash(powershell -Command:*)",
      "WebFetch(domain:www.ncdoi.gov)",
      "WebFetch(domain:harbor-ins.com)",
      "WebFetch(domain:www.carolinaattorneys.com)",
      "mcp__Desktop_Commander__get_file_info",
      "Bash(python:*)",
      "Bash(ls:*)",
      "Bash(dir:*)",
      "Bash(echo:*)",
      "Bash(python3:*)",
      "Bash(git pull:*)",
      "Bash(powershell -File:*)",
      "Bash(powershell -NoProfile -Command:*)",
      "Bash(powershell:*)",
      "Bash(curl:*)",
      "Bash(\"C:\\\\Users\\\\bill\\\\OneDrive\\\\Documents\\\\Bill-Layne-Insurance-Agency-LIVE\\\\blog\\\\temp\\\\extract_images.py\" << 'PYEOF'\nimport re\nimport base64\nimport os\nimport io\nfrom html.parser import HTMLParser\nfrom PIL import Image\n\nINPUT_FILE = r\"C:\\\\Users\\\\bill\\\\OneDrive\\\\Documents\\\\Blog Folders stored\\\\why-waiting-30-days-to-add-your-new-car-could-cost-you-thousands-the-complete-checklist.html\"\nOUTPUT_DIR = r\"C:\\\\Users\\\\bill\\\\OneDrive\\\\Documents\\\\Bill-Layne-Insurance-Agency-LIVE\\\\blog\\\\temp\"\n\n# ── Read the file ──────────────────────────────────────────────\nprint\\(f\"Reading: {INPUT_FILE}\"\\)\nwith open\\(INPUT_FILE, \"r\", encoding=\"utf-8\", errors=\"replace\"\\) as f:\n    html_content = f.read\\(\\)\n\nfile_size_kb = len\\(html_content.encode\\(\"utf-8\"\\)\\) / 1024\nprint\\(f\"File size: {file_size_kb:.1f} KB \\({len\\(html_content\\)} characters\\)\\\\n\"\\)\n\n# ── 1\\) Find all base64 image data URIs ────────────────────────\nprint\\(\"=\" * 70\\)\nprint\\(\"BASE64 IMAGE EXTRACTION\"\\)\nprint\\(\"=\" * 70\\)\n\n# Pattern to match data URIs inside src=\"...\" or src='...'\ndata_uri_pattern = re.compile\\(\n    r'data:image/\\(png|jpeg|jpg|gif|webp\\);base64,\\([A-Za-z0-9+/=\\\\s]+\\)',\n    re.IGNORECASE\n\\)\n\n# Find line numbers by searching line-by-line\nlines = html_content.split\\(\"\\\\n\"\\)\nimage_count = 0\nimages_info = []\n\nfor line_num, line in enumerate\\(lines, 1\\):\n    for match in data_uri_pattern.finditer\\(line\\):\n        image_count += 1\n        fmt = match.group\\(1\\).lower\\(\\)\n        b64_data = match.group\\(2\\).replace\\(\"\\\\n\", \"\"\\).replace\\(\"\\\\r\", \"\"\\).replace\\(\" \", \"\"\\)\n\n        # Decode the base64 data\n        try:\n            raw_bytes = base64.b64decode\\(b64_data\\)\n        except Exception as e:\n            print\\(f\"\\\\n  [img-{image_count}] ERROR decoding base64 on line {line_num}: {e}\"\\)\n            continue\n\n        raw_size_kb = len\\(raw_bytes\\) / 1024\n\n        # Determine output filename - always save as .jpg\n        out_name = f\"img-{image_count}.jpg\"\n        out_path = os.path.join\\(OUTPUT_DIR, out_name\\)\n\n        # Open image with Pillow and convert/save as JPEG\n        try:\n            img = Image.open\\(io.BytesIO\\(raw_bytes\\)\\)\n            orig_mode = img.mode\n            orig_size = img.size  # \\(width, height\\)\n\n            # Convert to RGB if needed \\(PNG may have RGBA\\)\n            if img.mode in \\(\"RGBA\", \"P\", \"LA\"\\):\n                # Create white background for transparency\n                bg = Image.new\\(\"RGB\", img.size, \\(255, 255, 255\\)\\)\n                if img.mode == \"P\":\n                    img = img.convert\\(\"RGBA\"\\)\n                bg.paste\\(img, mask=img.split\\(\\)[-1] if \"A\" in img.mode else None\\)\n                img = bg\n            elif img.mode != \"RGB\":\n                img = img.convert\\(\"RGB\"\\)\n\n            img.save\\(out_path, \"JPEG\", quality=85, optimize=True\\)\n            saved_size_kb = os.path.getsize\\(out_path\\) / 1024\n\n            print\\(f\"\\\\n  [{out_name}]\"\\)\n            print\\(f\"    Line number  : {line_num}\"\\)\n            print\\(f\"    Source format : {fmt.upper\\(\\)}\"\\)\n            print\\(f\"    Dimensions   : {orig_size[0]} x {orig_size[1]}\"\\)\n            print\\(f\"    Color mode   : {orig_mode}\"\\)\n            print\\(f\"    Raw b64 size : {raw_size_kb:.1f} KB\"\\)\n            print\\(f\"    Saved as JPEG: {saved_size_kb:.1f} KB\"\\)\n            if fmt == \"png\":\n                ratio = \\(1 - saved_size_kb / raw_size_kb\\) * 100 if raw_size_kb > 0 else 0\n                print\\(f\"    Compression  : {ratio:.0f}% smaller after PNG->JPEG conversion\"\\)\n\n            images_info.append\\({\n                \"index\": image_count,\n                \"line\": line_num,\n                \"format\": fmt,\n                \"dimensions\": orig_size,\n                \"saved_path\": out_path,\n                \"saved_size_kb\": saved_size_kb\n            }\\)\n\n        except Exception as e:\n            print\\(f\"\\\\n  [img-{image_count}] ERROR processing image on line {line_num}: {e}\"\\)\n            # Save raw bytes as fallback\n            with open\\(out_path.replace\\(\".jpg\", f\".{fmt}\"\\), \"wb\"\\) as fout:\n                fout.write\\(raw_bytes\\)\n            print\\(f\"    Saved raw file instead: {raw_size_kb:.1f} KB\"\\)\n\nprint\\(f\"\\\\n  Total base64 images found: {image_count}\"\\)\n\n# ── 2\\) Find context around each <img> tag ─────────────────────\nprint\\(\"\\\\n\" + \"=\" * 70\\)\nprint\\(\"IMAGE TAG CONTEXT \\(for identifying hero image\\)\"\\)\nprint\\(\"=\" * 70\\)\n\nimg_tag_pattern = re.compile\\(r'<img\\\\b[^>]*>', re.IGNORECASE | re.DOTALL\\)\nfor line_num, line in enumerate\\(lines, 1\\):\n    for m in img_tag_pattern.finditer\\(line\\):\n        tag = m.group\\(0\\)\n        # Only show tags that have base64 data \\(skip external URLs\\)\n        if \"base64\" in tag:\n            # Extract useful attributes\n            cls_match = re.search\\(r'class\\\\s*=\\\\s*[\"\\\\']\\([^\"\\\\']*\\)[\"\\\\']', tag\\)\n            id_match = re.search\\(r'id\\\\s*=\\\\s*[\"\\\\']\\([^\"\\\\']*\\)[\"\\\\']', tag\\)\n            alt_match = re.search\\(r'alt\\\\s*=\\\\s*[\"\\\\']\\([^\"\\\\']*\\)[\"\\\\']', tag\\)\n            style_match = re.search\\(r'style\\\\s*=\\\\s*[\"\\\\']\\([^\"\\\\']*\\)[\"\\\\']', tag\\)\n            width_match = re.search\\(r'width\\\\s*=\\\\s*[\"\\\\']?\\(\\\\d+\\)', tag\\)\n\n            # Get surrounding context \\(check parent elements on nearby lines\\)\n            context_start = max\\(0, line_num - 4\\)\n            context_end = min\\(len\\(lines\\), line_num + 2\\)\n            parent_context = []\n            for cl in range\\(context_start, context_end\\):\n                stripped = lines[cl].strip\\(\\)\n                if stripped and \"base64\" not in stripped and len\\(stripped\\) < 200:\n                    parent_context.append\\(f\"      L{cl+1}: {stripped[:150]}\"\\)\n\n            print\\(f\"\\\\n  <img> tag on line {line_num}:\"\\)\n            print\\(f\"    class : {cls_match.group\\(1\\) if cls_match else '\\(none\\)'}\"\\)\n            print\\(f\"    id    : {id_match.group\\(1\\) if id_match else '\\(none\\)'}\"\\)\n            print\\(f\"    alt   : {alt_match.group\\(1\\) if alt_match else '\\(none\\)'}\"\\)\n            print\\(f\"    style : {style_match.group\\(1\\)[:100] if style_match else '\\(none\\)'}\"\\)\n            print\\(f\"    width : {width_match.group\\(1\\) if width_match else '\\(none\\)'}\"\\)\n            if parent_context:\n                print\\(f\"    Surrounding HTML context:\"\\)\n                for pc in parent_context:\n                    print\\(pc\\)\n\n# ── 3\\) Extract visible text ───────────────────────────────────\nprint\\(\"\\\\n\" + \"=\" * 70\\)\nprint\\(\"TEXT CONTENT EXTRACTION\"\\)\nprint\\(\"=\" * 70\\)\n\nclass TextExtractor\\(HTMLParser\\):\n    def __init__\\(self\\):\n        super\\(\\).__init__\\(\\)\n        self.text_parts = []\n        self.skip_tags = {\"script\", \"style\", \"noscript\", \"head\", \"meta\", \"link\"}\n        self.current_skip = 0\n\n    def handle_starttag\\(self, tag, attrs\\):\n        if tag.lower\\(\\) in self.skip_tags:\n            self.current_skip += 1\n\n    def handle_endtag\\(self, tag\\):\n        if tag.lower\\(\\) in self.skip_tags:\n            self.current_skip = max\\(0, self.current_skip - 1\\)\n\n    def handle_data\\(self, data\\):\n        if self.current_skip == 0:\n            cleaned = data.strip\\(\\)\n            if cleaned:\n                self.text_parts.append\\(cleaned\\)\n\nextractor = TextExtractor\\(\\)\nextractor.feed\\(html_content\\)\nall_text = \" \".join\\(extractor.text_parts\\)\n\n# Clean up whitespace\nall_text = re.sub\\(r'\\\\s+', ' ', all_text\\).strip\\(\\)\n\nwords = all_text.split\\(\\)\ntotal_words = len\\(words\\)\nread_time_minutes = max\\(1, round\\(total_words / 238\\)\\)  # avg reading speed\n\nprint\\(f\"\\\\n  Total word count: {total_words}\"\\)\nprint\\(f\"  Estimated read time: {read_time_minutes} min \\(at 238 wpm\\)\\\\n\"\\)\n\n# First 500 words\nfirst_500 = \" \".join\\(words[:500]\\)\nprint\\(\"  FIRST 500 WORDS:\"\\)\nprint\\(\"  \" + \"-\" * 60\\)\n# Print in wrapped chunks\nchunk_size = 100\nfor i in range\\(0, len\\(first_500\\), chunk_size\\):\n    print\\(f\"  {first_500[i:i+chunk_size]}\"\\)\n\nprint\\(\"\\\\n\" + \"=\" * 70\\)\nprint\\(\"SUMMARY\"\\)\nprint\\(\"=\" * 70\\)\nprint\\(f\"  Images extracted : {image_count}\"\\)\nprint\\(f\"  Output directory : {OUTPUT_DIR}\"\\)\nprint\\(f\"  Total word count : {total_words}\"\\)\nprint\\(f\"  Read time        : {read_time_minutes} min\"\\)\nfor info in images_info:\n    print\\(f\"  img-{info['index']}.jpg : {info['dimensions'][0]}x{info['dimensions'][1]}, {info['saved_size_kb']:.1f} KB \\({info['format'].upper\\(\\)} source\\)\"\\)\nPYEOF)",
      "Bash(\"C:\\\\Users\\\\bill\\\\OneDrive\\\\Documents\\\\Bill-Layne-Insurance-Agency-LIVE\\\\replace_base64.py\" << 'PYTHON_SCRIPT'\nimport re\nimport os\n\n# Paths\nsource = r\"C:\\\\Users\\\\bill\\\\OneDrive\\\\Documents\\\\Blog Folders stored\\\\why-waiting-30-days-to-add-your-new-car-could-cost-you-thousands-the-complete-checklist.html\"\ndest = r\"C:\\\\Users\\\\bill\\\\OneDrive\\\\Documents\\\\Bill-Layne-Insurance-Agency-LIVE\\\\blog\\\\blogs\\\\why-waiting-30-days-to-add-your-new-car-could-cost-you-thousands.html\"\n\n# Imgur URLs in order of appearance\nimgur_urls = [\n    \"https://i.imgur.com/gvIQnIQ.jpeg\",\n    \"https://i.imgur.com/N6kXkMe.jpeg\",\n    \"https://i.imgur.com/iOY4k2V.jpeg\",\n]\n\n# Read source\nwith open\\(source, \"r\", encoding=\"utf-8\"\\) as f:\n    html = f.read\\(\\)\n\nprint\\(f\"Source file size: {len\\(html\\):,} bytes\"\\)\n\n# Find all base64 data URIs\nbase64_pattern = r'data:image/\\(?:png|jpeg|jpg|gif|webp\\);base64,[A-Za-z0-9+/=]+'\nmatches = list\\(re.finditer\\(base64_pattern, html\\)\\)\nprint\\(f\"Found {len\\(matches\\)} base64 image data URIs\"\\)\n\nfor i, m in enumerate\\(matches\\):\n    start = m.start\\(\\)\n    line_num = html[:start].count\\('\\\\n'\\) + 1\n    print\\(f\"  Match {i+1}: line ~{line_num}, length {len\\(m.group\\(\\)\\):,} chars\"\\)\n\nif len\\(matches\\) != 3:\n    print\\(f\"WARNING: Expected 3 base64 images, found {len\\(matches\\)}. Proceeding with min\\(found, 3\\).\"\\)\n\n# Replace base64 data URIs with Imgur URLs \\(in reverse order to preserve positions\\)\nfor i in range\\(min\\(len\\(matches\\), 3\\) - 1, -1, -1\\):\n    m = matches[i]\n    html = html[:m.start\\(\\)] + imgur_urls[i] + html[m.end\\(\\):]\n    print\\(f\"Replaced match {i+1} with {imgur_urls[i]}\"\\)\n\n# Now add image performance attributes\n# Find all <img tags\nimg_pattern = r'<img\\\\b[^>]*>'\nimg_matches = list\\(re.finditer\\(img_pattern, html, re.IGNORECASE | re.DOTALL\\)\\)\nprint\\(f\"\\\\nFound {len\\(img_matches\\)} <img> tags total\"\\)\n\n# We need to identify which img tags contain our Imgur URLs\ntarget_imgs = []\nfor m in img_matches:\n    tag = m.group\\(\\)\n    for url in imgur_urls:\n        if url in tag:\n            target_imgs.append\\(\\(m, url, imgur_urls.index\\(url\\)\\)\\)\n            break\n\nprint\\(f\"Found {len\\(target_imgs\\)} img tags with Imgur URLs\"\\)\n\n# Process in reverse order to preserve positions\nfor m, url, idx in reversed\\(target_imgs\\):\n    tag = m.group\\(\\)\n    new_tag = tag\n    \n    # Add width if not present\n    if 'width=' not in new_tag.lower\\(\\):\n        new_tag = new_tag.replace\\('<img ', '<img width=\"1344\" ', 1\\)\n    \n    # Add height if not present\n    if 'height=' not in new_tag.lower\\(\\):\n        new_tag = new_tag.replace\\('<img ', '<img height=\"768\" ', 1\\)\n    \n    # For image index 0 \\(hero\\): NO loading=\"lazy\" — remove it if present\n    if idx == 0:\n        # Remove loading=\"lazy\" if it exists on hero\n        new_tag = re.sub\\(r'\\\\s*loading\\\\s*=\\\\s*\"lazy\"', '', new_tag, flags=re.IGNORECASE\\)\n    else:\n        # For images 2 and 3: add loading=\"lazy\" if not present\n        if 'loading=' not in new_tag.lower\\(\\):\n            new_tag = new_tag.replace\\('<img ', '<img loading=\"lazy\" ', 1\\)\n    \n    if new_tag != tag:\n        html = html[:m.start\\(\\)] + new_tag + html[m.end\\(\\):]\n        print\\(f\"  Updated img {idx+1}: added attributes\"\\)\n    else:\n        print\\(f\"  Img {idx+1}: no changes needed \\(attributes already present\\)\"\\)\n\n# Save output\nos.makedirs\\(os.path.dirname\\(dest\\), exist_ok=True\\)\nwith open\\(dest, \"w\", encoding=\"utf-8\"\\) as f:\n    f.write\\(html\\)\n\n# === VERIFICATION ===\nprint\\(\"\\\\n\" + \"=\"*60\\)\nprint\\(\"VERIFICATION\"\\)\nprint\\(\"=\"*60\\)\n\n# File size\nfile_size = os.path.getsize\\(dest\\)\nprint\\(f\"\\\\n1. Output file size: {file_size:,} bytes\"\\)\n\n# Check no base64 remains\nwith open\\(dest, \"r\", encoding=\"utf-8\"\\) as f:\n    content = f.read\\(\\)\n\nremaining = re.findall\\(r'data:image/', content\\)\nif remaining:\n    print\\(f\"\\\\n2. WARNING: Found {len\\(remaining\\)} remaining 'data:image' strings!\"\\)\nelse:\n    print\\(f\"\\\\n2. PASS: No 'data:image' strings remain in the output file.\"\\)\n\n# Confirm Imgur URLs\nprint\\(f\"\\\\n3. Imgur URL check:\"\\)\nfor url in imgur_urls:\n    count = content.count\\(url\\)\n    status = \"PASS\" if count > 0 else \"FAIL\"\n    print\\(f\"   {status}: {url} found {count} time\\(s\\)\"\\)\n\n# Print img tags\nprint\\(f\"\\\\n4. All <img> tags in output:\"\\)\nfor i, m in enumerate\\(re.finditer\\(r'<img\\\\b[^>]*>', content, re.IGNORECASE | re.DOTALL\\)\\):\n    tag = m.group\\(\\)\n    # Truncate very long tags for display\n    if len\\(tag\\) > 300:\n        tag = tag[:300] + \"...\"\n    line_num = content[:m.start\\(\\)].count\\('\\\\n'\\) + 1\n    print\\(f\"   img #{i+1} \\(line ~{line_num}\\): {tag}\"\\)\n\nprint\\(f\"\\\\nOutput saved to: {dest}\"\\)\nPYTHON_SCRIPT)",
      "Bash(\"C:\\\\Users\\\\bill\\\\OneDrive\\\\Documents\\\\Bill-Layne-Insurance-Agency-LIVE\\\\update_blog_data.py\" << 'PYEOF'\nimport json\nimport sys\n\nBASE = r\"C:\\\\Users\\\\bill\\\\OneDrive\\\\Documents\\\\Bill-Layne-Insurance-Agency-LIVE\"\nBLOGS_JSON = BASE + r\"\\\\blog\\\\data\\\\blogs.json\"\nINDEX_HTML = BASE + r\"\\\\blog\\\\index.html\"\n\n# 1. Read blogs.json\nprint\\(\"Step 1: Reading blogs.json ...\"\\)\nwith open\\(BLOGS_JSON, \"r\", encoding=\"utf-8\"\\) as f:\n    blogs_data = json.load\\(f\\)\n\nblog_count = len\\(blogs_data\\)\nprint\\(f\"  -> Loaded {blog_count} blog entries from blogs.json\"\\)\n\n# 2. Minify the JSON\nprint\\(\"Step 2: Minifying JSON ...\"\\)\nminified = json.dumps\\(blogs_data, separators=\\(\",\", \":\"\\), ensure_ascii=False\\)\nprint\\(f\"  -> Minified JSON length: {len\\(minified\\)} characters\"\\)\n\n# 3. Read index.html\nprint\\(\"Step 3: Reading index.html ...\"\\)\nwith open\\(INDEX_HTML, \"r\", encoding=\"utf-8\"\\) as f:\n    lines = f.readlines\\(\\)\n\ntotal_lines = len\\(lines\\)\nprint\\(f\"  -> Read {total_lines} lines from index.html\"\\)\n\n# 4. Find line 897 \\(0-indexed: 896\\)\ntarget_index = 896\nprint\\(f\"Step 4: Checking line 897 \\(index {target_index}\\) ...\"\\)\nif target_index >= total_lines:\n    print\\(f\"  ERROR: index.html only has {total_lines} lines, cannot access line 897\"\\)\n    sys.exit\\(1\\)\n\nold_line = lines[target_index]\nif \"window.__BLOG_DATA__\" in old_line:\n    print\\(f\"  -> Found 'window.__BLOG_DATA__' on line 897\"\\)\n    print\\(f\"  -> Old line length: {len\\(old_line\\)} characters\"\\)\nelse:\n    # Search nearby lines\n    print\\(f\"  WARNING: 'window.__BLOG_DATA__' NOT found on line 897.\"\\)\n    print\\(f\"  -> Line 897 content \\(first 200 chars\\): {old_line[:200]}\"\\)\n    found = False\n    for i in range\\(max\\(0, target_index - 10\\), min\\(total_lines, target_index + 10\\)\\):\n        if \"window.__BLOG_DATA__\" in lines[i]:\n            print\\(f\"  -> Found it on line {i+1} instead!\"\\)\n            target_index = i\n            old_line = lines[target_index]\n            found = True\n            break\n    if not found:\n        print\\(\"  ERROR: Could not find 'window.__BLOG_DATA__' anywhere nearby. Aborting.\"\\)\n        sys.exit\\(1\\)\n\n# 5. Replace that line\nprint\\(f\"Step 5: Replacing line {target_index + 1} with updated __BLOG_DATA__ ...\"\\)\nnew_line = f\"<script>window.__BLOG_DATA__ = {minified};</script>\\\\n\"\nlines[target_index] = new_line\nprint\\(f\"  -> New line length: {len\\(new_line\\)} characters\"\\)\n\n# 6. Write updated index.html\nprint\\(\"Step 6: Writing updated index.html ...\"\\)\nwith open\\(INDEX_HTML, \"w\", encoding=\"utf-8\"\\) as f:\n    f.writelines\\(lines\\)\nprint\\(\"  -> Done writing.\"\\)\n\n# 7. Verify by counting \"id\": occurrences on the new line\nprint\\(\"Step 7: Verifying ...\"\\)\nid_count = new_line.count\\('\"id\":'\\)\nprint\\(f\"  -> Occurrences of '\\\\\"id\\\\\":' on line {target_index + 1}: {id_count}\"\\)\nif id_count == blog_count:\n    print\\(f\"  -> MATCH: {id_count} == {blog_count} \\(blogs.json count\\)\"\\)\nelse:\n    print\\(f\"  -> MISMATCH: expected {blog_count}, got {id_count}\"\\)\n\n# 8. Print first 200 chars of new line\nprint\\(f\"Step 8: First 200 chars of line {target_index + 1}:\"\\)\nprint\\(f\"  {new_line[:200]}\"\\)\n\nprint\\(\"\\\\nAll done!\"\\)\nPYEOF)",
      "Bash(find:*)",
      "Bash(for f in blog/blogs/*.html)",
      "Bash(do sed -i 's|og:url\"\" content=\"\"https://billlayneinsurance.com|og:url\"\" content=\"\"https://www.billlayneinsurance.com|g' \"$f\")",
      "Bash(done)",
      "Bash(gh run list:*)",
      "Bash(gh api:*)",
      "mcp__Windows-MCP__Shell",
      "mcp__Windows-MCP__Wait",
      "Bash(timeout:*)",
      "Bash(wc:*)",
      "Bash(while read f)",
      "Bash(do if ! grep -q \"tailwind.config\" \"$f\")",
      "Bash(then echo \"$f\")",
      "Bash(fi)",
      "Bash(do if grep -q \"tailwind.config\" \"$f\")",
      "Bash(\"C:/Users/bill/OneDrive/Documents/Bill-Layne-Insurance-Agency-LIVE/tools/tailwindcss.exe\" --help)",
      "Bash(./tools/tailwindcss.exe:*)",
      "Bash(git status:*)",
      "Bash(tools/tailwindcss.exe:*)",
      "Bash(cygpath:*)",
      "Bash(for f in nationwide-opt.png progressive-opt.png travelers-opt.png national-general-opt.png alamance-opt.png rosa-opt.jpg)",
      "Bash(do echo -n \"Uploading $f... \")",
      "mcp__Desktop_Commander__read_multiple_files",
      "mcp__Desktop_Commander__start_process",
      "mcp__Desktop_Commander__read_process_output",
      "Bash(xargs:*)",
      "mcp__Claude_Preview__preview_start",
      "Bash(git -C /c/Users/bill/OneDrive/Documents/Bill-Layne-Insurance-Agency-LIVE/.claude/worktrees/romantic-pike log --oneline -20)",
      "Bash(git -C /c/Users/bill/OneDrive/Documents/Bill-Layne-Insurance-Agency-LIVE/.claude/worktrees/romantic-pike status)",
      "Bash(printenv:*)",
      "Bash(gh pr create:*)",
      "Bash(gh auth status:*)",
      "Bash(gh:*)",
      "Bash(\"C:\\\\Users\\\\bill\\\\OneDrive\\\\Documents\\\\Bill-Layne-Insurance-Agency-LIVE\\\\tools\\\\tailwindcss.exe\" -i css/tailwind-input.css -o css/tailwind-output.css --minify -c js/tailwind.config.js)"
    ]
  }
}
